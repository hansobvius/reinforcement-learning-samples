import numpy as np
import time
import os

# --- PATH CONFIGURATION ---
# Define paths for data and models
BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
DATA_DIR = os.path.join(BASE_DIR, 'data')
MODELS_DIR = os.path.join(BASE_DIR, 'models')

# Ensure directories exist
os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(MODELS_DIR, exist_ok=True)

def train_model():
    # --- CONFIGURA√á√ÉO ---
    # 6 estados (pisos 0 a 5)
    # 2 a√ß√µes (0: esquerda, 1: direita)
    TAMANHO_CORREDOR = 6 
    POSICAO_DIAMANTE = 5 
    
    # Caminho para o modelo salvo
    model_path = os.path.join(MODELS_DIR, 'q_table_corridor.npy')
    
    # Tenta carregar o c√©rebro se ele j√° existir, sen√£o cria um novo
    if os.path.exists(model_path):
        print("üß† C√©rebro antigo encontrado! Carregando dados...")
        q_table = np.load(model_path)
    else:
        print("üÜï Nenhum c√©rebro antigo encontrado. Criando um novo com zeros...")
        # Criando a Tabela Q (C√©rebro) com zeros
        # Linhas = Estados, Colunas = A√ß√µes
        q_table = np.zeros((TAMANHO_CORREDOR, 2)) 
    
    # Par√¢metros de Aprendizado
    alpha = 0.5  # Taxa de aprendizado (qu√£o r√°pido ele aceita novas verdades)
    gamma = 0.9  # Fator de desconto (vis√£o de futuro)
    epsilon = 0.1 # Chance de fazer loucura (explora√ß√£o)
    
    print("--- IN√çCIO DO TREINO ---")
    
    # Vamos treinar por 10 epis√≥dios (tentativas)
    training_log = [] # List to store log data
    
    for episodio in range(10):
        estado_atual = 0 # Rob√¥ come√ßa sempre na esquerda
        passos = 0
        game_over = False
        total_reward = 0
    
        print(f"\nEpis√≥dio {episodio + 1}: Rob√¥ acordou no 0.")
    
        while not game_over:
            # 1. Escolher A√ß√£o (Estrat√©gia do 'Epsilon-Greedy')
            if np.random.uniform(0, 1) < epsilon:
                acao = np.random.choice([0, 1]) # 10% de chance de agir aleat√≥rio
            else:
                acao = np.argmax(q_table[estado_atual]) # 90% de chance de usar o c√©rebro
    
            # 2. Executar A√ß√£o e ver o resultado
            # Se a√ß√£o for 1 (direita), aumenta posi√ß√£o. Se 0 (esquerda), diminui.
            novo_estado = estado_atual + 1 if acao == 1 else estado_atual - 1
            
            # Regras de f√≠sica (paredes)
            if novo_estado < 0:
                novo_estado = 0 # Bateu na parede esquerda
            elif novo_estado > 5:
                novo_estado = 5 # N√£o pode passar do diamante
    
            # 3. Definir Recompensa
            reward = 0
            if novo_estado == POSICAO_DIAMANTE:
                reward = 10 # ACHOU O DIAMANTE!
                game_over = True
            
            # 4. ATUALIZAR O C√âREBRO (A M√°gica da F√≥rmula de Bellman simplificada)
            # Valor Antigo
            antigo_valor = q_table[estado_atual, acao]
            
            # O melhor valor do pr√≥ximo estado (Olhando para o futuro)
            melhor_futuro = np.max(q_table[novo_estado])
            
            # C√°lculo do novo valor
            novo_valor = antigo_valor + alpha * (reward + gamma * melhor_futuro - antigo_valor)
            
            # Escreve na tabela
            q_table[estado_atual, acao] = novo_valor
    
            # Move o rob√¥ fisicamente
            estado_atual = novo_estado
            passos += 1
            total_reward += reward
    
        # Log the episode data
        training_log.append(f"{episodio+1},{passos},{total_reward}\n")
        print(f"  -> Chegou no diamante em {passos} passos!")
    
    print("\n--- TREINO CONCLU√çDO ---")
    print("Veja o que a IA aprendeu (Tabela Q):")
    print("Linha = Piso (0 a 5) | Coluna 0 = Esq | Coluna 1 = Dir")
    print(q_table)
    
    # --- SAVING RESULTS ---
    # 1. Save the "Brain" (Model)
    np.save(model_path, q_table)
    print(f"\n[MODEL] Brain saved to: {model_path}")
    
    # 2. Save the Logs (Data)
    log_path = os.path.join(DATA_DIR, 'training_log.csv')
    with open(log_path, 'w') as f:
        f.write("Episode,Steps,TotalReward\n") # Header
        f.writelines(training_log)
    print(f"[DATA] Training logs saved to: {log_path}")

if __name__ == "__main__":
    train_model()
